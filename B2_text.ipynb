{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83b1fb26",
   "metadata": {},
   "source": [
    "\n",
    "## üß© **Assignment 2 ‚Äî Email Spam Classification**\n",
    "\n",
    "### üéØ **Title**\n",
    "\n",
    "> Classify the email using binary classification.\n",
    "> Use **K-Nearest Neighbors (KNN)** and **Support Vector Machine (SVM)** for classification and compare their performance.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Objective**\n",
    "\n",
    "To build and evaluate a **machine learning classifier** that detects whether an email is **Spam (1)** or **Not Spam (0)** using:\n",
    "\n",
    "* **K-Nearest Neighbors (KNN)**\n",
    "* **Support Vector Machine (SVM)**\n",
    "\n",
    "---\n",
    "\n",
    "## üìä **Dataset Details**\n",
    "\n",
    "**Source:** [Kaggle: Email Spam Classification Dataset](https://www.kaggle.com/datasets/balaka18/email-spam-classification-dataset-csv)\n",
    "\n",
    "| Attribute            | Description                                   |\n",
    "| -------------------- | --------------------------------------------- |\n",
    "| `Email No.`          | Unique email ID (numeric name)                |\n",
    "| `word1` ‚Äì `word3000` | Frequency count of top 3000 most common words |\n",
    "| `label`              | Target variable: 1 = spam, 0 = not spam       |\n",
    "\n",
    "* Total rows: **5172**\n",
    "* Total columns: **3002 (1 ID + 3000 word features + 1 label)**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è **Theory You Must Know**\n",
    "\n",
    "### 1Ô∏è‚É£ **Data Preprocessing**\n",
    "\n",
    "Preparing raw data before applying models:\n",
    "\n",
    "* Remove irrelevant features (like Email ID)\n",
    "* Handle missing/null values\n",
    "* Normalize/scale data (important for KNN and SVM)\n",
    "* Split into train and test data\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ **Binary Classification**\n",
    "\n",
    "Binary classification predicts **two categories** (e.g., spam or not spam).\n",
    "\n",
    "Model output:\n",
    "\n",
    "```\n",
    "1 ‚Üí Spam\n",
    "0 ‚Üí Not Spam\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ **K-Nearest Neighbors (KNN)**\n",
    "\n",
    "| Concept         | Explanation                                                                    |\n",
    "| --------------- | ------------------------------------------------------------------------------ |\n",
    "| Type            | Supervised Learning (Classification)                                           |\n",
    "| Idea            | A data point is classified by the **majority vote** of its K nearest neighbors |\n",
    "| Distance Metric | Usually **Euclidean Distance**                                                 |\n",
    "| Formula         | ( d = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \\ldots} )                          |\n",
    "| Hyperparameter  | ‚Äòk‚Äô = number of neighbors to consider                                          |\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "* Simple and intuitive\n",
    "* Works well with clean data\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "* Slow for large datasets\n",
    "* Needs scaling\n",
    "* Sensitive to irrelevant features\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ **Support Vector Machine (SVM)**\n",
    "\n",
    "| Concept     | Explanation                                                |\n",
    "| ----------- | ---------------------------------------------------------- |\n",
    "| Type        | Supervised Learning (Classification)                       |\n",
    "| Idea        | Finds a **hyperplane** that best separates the two classes |\n",
    "| Key Concept | **Maximizes margin** between spam and not-spam             |\n",
    "| Kernels     | Linear, RBF (Radial Basis Function), Polynomial, etc.      |\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "* Works well with high-dimensional data (like word frequencies)\n",
    "* Handles non-linear separation using kernel trick\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "* Slower on very large datasets\n",
    "* Needs proper parameter tuning\n",
    "\n",
    "---\n",
    "\n",
    "### 5Ô∏è‚É£ **Model Evaluation Metrics**\n",
    "\n",
    "| Metric               | Meaning                             | Ideal Value                  |\n",
    "| -------------------- | ----------------------------------- | ---------------------------- |\n",
    "| **Accuracy**         | (TP + TN) / (Total)                 | Closer to 1                  |\n",
    "| **Precision**        | TP / (TP + FP)                      | High ‚Üí fewer false positives |\n",
    "| **Recall**           | TP / (TP + FN)                      | High ‚Üí fewer false negatives |\n",
    "| **F1 Score**         | Harmonic mean of Precision & Recall | High = balanced model        |\n",
    "| **Confusion Matrix** | Table showing prediction results    | 2√ó2 matrix (TP, TN, FP, FN)  |\n",
    "\n",
    "---\n",
    "\n",
    "## üíª **Typical Code Flow in Your Notebook (B2.ipynb)**\n",
    "\n",
    "### Step 1: Import Libraries\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Load Dataset\n",
    "\n",
    "```python\n",
    "df = pd.read_csv('emails.csv')\n",
    "df.head()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Data Preprocessing\n",
    "\n",
    "```python\n",
    "# Drop ID column\n",
    "df = df.drop('Email No.', axis=1)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Feature Scaling\n",
    "\n",
    "KNN and SVM are **distance-based algorithms**, so feature scaling is mandatory.\n",
    "\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Apply **KNN Classifier**\n",
    "\n",
    "```python\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6: Apply **SVM Classifier**\n",
    "\n",
    "```python\n",
    "svm = SVC(kernel='linear')  # or kernel='rbf'\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 7: Evaluate Models\n",
    "\n",
    "```python\n",
    "print(\"KNN Accuracy:\", accuracy_score(y_test, y_pred_knn))\n",
    "print(confusion_matrix(y_test, y_pred_knn))\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(confusion_matrix(y_test, y_pred_svm))\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ **Typical Results**\n",
    "\n",
    "| Model            | Accuracy | Precision | Recall | F1-Score |\n",
    "| ---------------- | -------- | --------- | ------ | -------- |\n",
    "| **KNN (k=5)**    | ~0.92    | 0.91      | 0.93   | 0.92     |\n",
    "| **SVM (Linear)** | ~0.96    | 0.95      | 0.96   | 0.95     |\n",
    "\n",
    "‚úÖ **Conclusion:**\n",
    "SVM performs slightly better than KNN due to its ability to handle high-dimensional (word-based) data efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© **Viva Questions You Might Be Asked**\n",
    "\n",
    "| Question                          | Short Answer                                                                         |\n",
    "| --------------------------------- | ------------------------------------------------------------------------------------ |\n",
    "| What is KNN?                      | Instance-based ML algorithm that classifies based on nearest neighbors.              |\n",
    "| What is SVM?                      | Algorithm that separates classes using the best hyperplane.                          |\n",
    "| Why scale features?               | To give equal weight to all features (distance-based models are sensitive to scale). |\n",
    "| What is Kernel in SVM?            | Function that transforms data into higher dimensions for better separation.          |\n",
    "| What is binary classification?    | Predicting one of two categories (spam or not spam).                                 |\n",
    "| What is Confusion Matrix?         | Table showing true and false predictions for each class.                             |\n",
    "| Why does SVM perform better here? | High-dimensional sparse data is handled well by SVM.                                 |\n",
    "| What is F1 score?                 | Balance between precision and recall.                                                |\n",
    "| How do you choose ‚Äòk‚Äô in KNN?     | Try multiple values; choose the one with best validation accuracy.                   |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Possible ‚ÄúModification‚Äù Questions During Viva**\n",
    "\n",
    "| Asked To Do                          | How To Do It                                       |\n",
    "| ------------------------------------ | -------------------------------------------------- |\n",
    "| ‚ÄúTry another kernel‚Äù                 | `SVC(kernel='rbf')` or `'poly'`                    |\n",
    "| ‚ÄúChange value of k‚Äù                  | `KNeighborsClassifier(n_neighbors=7)`              |\n",
    "| ‚ÄúShow confusion matrix as a heatmap‚Äù | `sns.heatmap(confusion_matrix(...))`               |\n",
    "| ‚ÄúShow F1 Score only‚Äù                 | `from sklearn.metrics import f1_score`             |\n",
    "| ‚ÄúUse cross-validation‚Äù               | `cross_val_score(model, X, y, cv=5)`               |\n",
    "| ‚ÄúNormalize instead of standardize‚Äù   | Use `MinMaxScaler()` instead of `StandardScaler()` |\n",
    "\n",
    "---\n",
    "\n",
    "## üìà **Visualization Ideas (Optional but Good for Marks)**\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_svm), annot=True, fmt='d', cmap='coolwarm')\n",
    "plt.title(\"SVM Confusion Matrix\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß© **Key Takeaways**\n",
    "\n",
    "* **KNN** works on **distance similarity**, best for simple patterns.\n",
    "* **SVM** separates classes using a **hyperplane**, better for text-like, high-dimensional data.\n",
    "* **Scaling** is **mandatory** for both.\n",
    "* Evaluate with **Accuracy, Precision, Recall, F1**.\n",
    "* SVM generally outperforms KNN for spam datasets.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to generate a **1-page practical exam revision sheet (PDF-style)** for this Assignment B2 ‚Äî with:\n",
    "\n",
    "* Key definitions\n",
    "* Theory formulas\n",
    "* Common viva questions\n",
    "* Code snippets\n",
    "* Expected output summary?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4779a83",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
