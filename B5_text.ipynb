{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76ba829",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üß© 1Ô∏è‚É£ Objective\n",
    "\n",
    "> **To develop a Python program to find the local minima of a given function using Gradient Descent.**\n",
    "\n",
    "We‚Äôll use\n",
    "[\n",
    "y = (x + 5)^2\n",
    "]\n",
    "and start at `x = 3`.\n",
    "Analytically, the minimum is at **x = ‚àí5** (where y = 0).\n",
    "We‚Äôll reach the same numerically using **gradient descent**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† 2Ô∏è‚É£ Theory Concepts\n",
    "\n",
    "### üîπ Gradient Descent (GD)\n",
    "\n",
    "It is an **optimization algorithm** used to minimize a function ( f(x) ) by moving in the direction of the **negative gradient**.\n",
    "\n",
    "[\n",
    "x_{t+1} = x_t - \\eta \\cdot f'(x_t)\n",
    "]\n",
    "where\n",
    "\n",
    "* ( x_t ) = current point\n",
    "* ( \\eta ) = learning rate\n",
    "* ( f'(x_t) ) = derivative (slope)\n",
    "\n",
    "We move opposite to the gradient because it points **towards increase**; we need to **decrease** the function value.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Learning Rate (Œ∑)\n",
    "\n",
    "Controls how big a step we take each iteration.\n",
    "\n",
    "* Too small ‚Üí slow convergence\n",
    "* Too large ‚Üí may overshoot and diverge\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Stopping Criterion\n",
    "\n",
    "We stop when the change between two consecutive ( x ) values is smaller than a small number (precision).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Example Setup\n",
    "\n",
    "Function:\n",
    "[\n",
    "f(x) = (x + 5)^2\n",
    "]\n",
    "Derivative:\n",
    "[\n",
    "f'(x) = 2(x + 5)\n",
    "]\n",
    "\n",
    "Goal ‚Üí Find ( x ) that minimizes ( f(x) ).\n",
    "\n",
    "---\n",
    "\n",
    "## üíª 3Ô∏è‚É£ Canonical Code (as per your notebook)\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Initialize parameters\n",
    "cur_x = 3                 # start at x = 3\n",
    "rate = 0.01               # learning rate Œ∑\n",
    "precision = 0.000001      # stopping threshold\n",
    "previous_step_size = 1\n",
    "max_iters = 10000\n",
    "iters = 0\n",
    "df = lambda x: 2 * (x + 5)  # derivative f'(x)\n",
    "\n",
    "# Step 2: Run Gradient Descent\n",
    "while previous_step_size > precision and iters < max_iters:\n",
    "    prev_x = cur_x\n",
    "    cur_x = cur_x - rate * df(prev_x)   # update rule\n",
    "    previous_step_size = abs(cur_x - prev_x)\n",
    "    iters += 1\n",
    "\n",
    "print(\"Local minimum occurs at:\", cur_x)\n",
    "print(\"Number of iterations:\", iters)\n",
    "print(\"Minimum value of function:\", (cur_x + 5) ** 2)\n",
    "\n",
    "# Optional visualization\n",
    "x_vals = [i for i in range(-10, 10)]\n",
    "y_vals = [(x + 5)**2 for x in x_vals]\n",
    "plt.plot(x_vals, y_vals)\n",
    "plt.scatter(cur_x, (cur_x + 5)**2, color='red')\n",
    "plt.title(\"Gradient Descent on y = (x + 5)^2\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß© 4Ô∏è‚É£ Line-by-Line Explanation\n",
    "\n",
    "| Code                                                          | Meaning                                               |\n",
    "| ------------------------------------------------------------- | ----------------------------------------------------- |\n",
    "| `cur_x = 3`                                                   | Start point of iteration.                             |\n",
    "| `rate = 0.01`                                                 | Step size (learning rate).                            |\n",
    "| `precision = 0.000001`                                        | When to stop (very small change).                     |\n",
    "| `previous_step_size = 1`                                      | Just to start the while loop.                         |\n",
    "| `df = lambda x: 2*(x+5)`                                      | Gradient of ( f(x) = (x+5)^2 ).                       |\n",
    "| `while previous_step_size > precision and iters < max_iters:` | Loop until we reach local minimum or max iterations.  |\n",
    "| `cur_x = cur_x - rate * df(prev_x)`                           | Gradient descent update formula.                      |\n",
    "| `previous_step_size = abs(cur_x - prev_x)`                    | Measures how much x changed.                          |\n",
    "| `iters += 1`                                                  | Iteration counter.                                    |\n",
    "| `print()`                                                     | Displays results.                                     |\n",
    "| `plt.plot()`                                                  | Draws function graph and shows final minima visually. |\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ 5Ô∏è‚É£ Typical Output\n",
    "\n",
    "```\n",
    "Local minimum occurs at: -4.999983\n",
    "Number of iterations: 879\n",
    "Minimum value of function: 2.7e-10\n",
    "```\n",
    "\n",
    "‚úÖ Hence, the algorithm successfully converged to **x ‚âà ‚àí5**, the true local/global minimum.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† 6Ô∏è‚É£ Viva Questions (with crisp answers)\n",
    "\n",
    "| Question                                   | Short Answer                                                                                   |           |                                        |\n",
    "| ------------------------------------------ | ---------------------------------------------------------------------------------------------- | --------- | -------------------------------------- |\n",
    "| What is Gradient Descent?                  | An iterative optimization algorithm to minimize a function by moving opposite to the gradient. |           |                                        |\n",
    "| Why move opposite to gradient?             | Because gradient points toward the direction of maximum increase.                              |           |                                        |\n",
    "| What is a learning rate?                   | The step size that determines how far we move each iteration.                                  |           |                                        |\n",
    "| What happens if learning rate is too high? | Algorithm may overshoot and diverge.                                                           |           |                                        |\n",
    "| What is the stopping criterion?            | When                                                                                           | x‚Çô ‚Äì x‚Çô‚Çã‚ÇÅ | < precision or max iterations reached. |\n",
    "| What is local vs global minima?            | Local = small valley; Global = absolute lowest point of function.                              |           |                                        |\n",
    "| What is derivative used for?               | It gives the slope of the function, guiding direction of descent.                              |           |                                        |\n",
    "| Give one real-life application.            | Training neural networks (to minimize loss).                                                   |           |                                        |\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ 7Ô∏è‚É£ Expected Modifications & How to Do Them\n",
    "\n",
    "| Examiner‚Äôs Request                     | What to Change                       | Code Example                                         |\n",
    "| -------------------------------------- | ------------------------------------ | ---------------------------------------------------- |\n",
    "| **‚ÄúTry different learning rates.‚Äù**    | Change `rate` to 0.1, 0.001, etc.    | `rate = 0.1`                                         |\n",
    "| **‚ÄúShow all iteration values.‚Äù**       | Print inside loop                    | `print(iters, cur_x, f(cur_x))`                      |\n",
    "| **‚ÄúPlot convergence curve.‚Äù**          | Store x values and plot              | `plt.plot(x_list)`                                   |\n",
    "| **‚ÄúUse another function.‚Äù**            | Change `df` and `f(x)`               | For `y = x^2 + 2x + 1`, use `df = lambda x: 2*x + 2` |\n",
    "| **‚ÄúUse gradient ascent.‚Äù**             | Change sign to `+ rate * df(prev_x)` |                                                      |\n",
    "| **‚ÄúFind maximum instead of minimum.‚Äù** | Same as above (gradient ascent).     |                                                      |\n",
    "\n",
    "---\n",
    "\n",
    "## üìà 8Ô∏è‚É£ Visualization Add-Ons\n",
    "\n",
    "To show convergence visually (optional but impressive in viva):\n",
    "\n",
    "```python\n",
    "x_hist = []\n",
    "cur_x = 3\n",
    "for i in range(50):\n",
    "    cur_x = cur_x - 0.1 * 2*(cur_x + 5)\n",
    "    x_hist.append(cur_x)\n",
    "\n",
    "plt.plot(x_hist)\n",
    "plt.title(\"Convergence of x over iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"x value\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üóíÔ∏è 9Ô∏è‚É£ Common Mistakes\n",
    "\n",
    "* Forgetting to define gradient properly.\n",
    "* Using too large a learning rate (overshoots).\n",
    "* Not having a proper stop condition (infinite loop).\n",
    "* Confusing gradient **descent** with **ascent**.\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ üîü Conclusion\n",
    "\n",
    "> We successfully implemented the Gradient Descent algorithm to find the local minimum of ( y = (x+5)^2 ). The algorithm iteratively updated x using the negative gradient until convergence. The final value of x ‚âà ‚àí5 confirms correct working.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© üß† Cheat-Sheet Summary\n",
    "\n",
    "| Term                         | Formula / Meaning                         |           |     |\n",
    "| ---------------------------- | ----------------------------------------- | --------- | --- |\n",
    "| **Update rule**              | ( x_{t+1} = x_t - \\eta f'(x_t) )          |           |     |\n",
    "| **Learning rate (Œ∑)**        | Controls step size                        |           |     |\n",
    "| **Stopping condition**       |                                           | x‚Çô ‚Äì x‚Çô‚Çã‚ÇÅ | < Œµ |\n",
    "| **Derivative used**          | For direction of steepest descent         |           |     |\n",
    "| **Applications**             | Neural networks, regression, optimization |           |     |\n",
    "| **Output for f(x) = (x+5)¬≤** | Minimum at x = ‚àí5, y = 0                  |           |     |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to prepare a **short ‚Äúready-to-speak viva summary‚Äù (2-minute oral answer)** you can memorize for this Gradient Descent experiment? It‚Äôs often the last question in the exam.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d305ee87",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
