{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74ed81dd",
   "metadata": {},
   "source": [
    "Perfect ğŸ‘ â€” your uploaded file **B3.ipynb** corresponds to **Part B â€“ Assignment 3** in your LP3 manual:\n",
    "\n",
    "> ğŸ§  **â€œGiven a bank customer, build a neural-network-based classifier that can determine whether they will leave or not in the next 6 months.â€**\n",
    "\n",
    "Below is a **complete, exam-ready explanation** covering:\n",
    "\n",
    "* Full theory from the manual\n",
    "* Concepts used (ANN / Keras / TensorFlow / Normalization / Confusion Matrix)\n",
    "* Step-by-step code explanation (line by line)\n",
    "* All possible viva and â€œdo this changeâ€ questions with quick code fixes\n",
    "* Expected output & interpretation\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© 1ï¸âƒ£ Objective\n",
    "\n",
    "To design an **Artificial Neural Network (ANN)** model that predicts **customer churn** â€” whether a bank customer will **leave (1)** or **stay (0)**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“˜ 2ï¸âƒ£ Theory Concepts (as per manual)\n",
    "\n",
    "### ğŸ”¹ Artificial Neural Network (ANN)\n",
    "\n",
    "An **ANN** is inspired by the human brain.\n",
    "It has **neurons (nodes)** organized in layers:\n",
    "\n",
    "| Layer               | Purpose                                                                     |\n",
    "| ------------------- | --------------------------------------------------------------------------- |\n",
    "| **Input Layer**     | Receives features like CreditScore, Age, Balance, etc.                      |\n",
    "| **Hidden Layer(s)** | Extract patterns & relationships using weighted sums + activation functions |\n",
    "| **Output Layer**    | Produces final prediction (0 = stay, 1 = leave)                             |\n",
    "\n",
    "ğŸ§® Each neuron computes\n",
    "[\n",
    "y = f(Wx + b)\n",
    "]\n",
    "where **W = weights**, **b = bias**, and **f = activation function** (e.g. ReLU or Sigmoid).\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Keras & TensorFlow\n",
    "\n",
    "* **TensorFlow** â†’ Low-level numerical backend (from Google).\n",
    "* **Keras** â†’ High-level API (in Python) that runs on top of TensorFlow, simplifying model building.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Normalization (Min-Max Scaling)\n",
    "\n",
    "Ensures all features share a common scale:\n",
    "[\n",
    "X' = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
    "]\n",
    "Typical range = [0, 1]\n",
    "This helps gradient descent converge faster.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Confusion Matrix & Metrics\n",
    "\n",
    "| Term          | Formula             | Meaning                       |\n",
    "| ------------- | ------------------- | ----------------------------- |\n",
    "| **Accuracy**  | (TP + TN) / Total   | Overall correctness           |\n",
    "| **Precision** | TP / (TP + FP)      | Correct positive predictions  |\n",
    "| **Recall**    | TP / (TP + FN)      | Ability to find all positives |\n",
    "| **F1 Score**  | 2 Ã— (P Ã— R)/(P + R) | Balance of precision & recall |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’» 3ï¸âƒ£ Code Explanation (Line by Line)\n",
    "\n",
    "Below is the canonical B3 notebook implementation (matches your file).\n",
    "\n",
    "```python\n",
    "# 1. Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "\n",
    "* `pandas`, `numpy` â†’ data handling\n",
    "* `LabelEncoder` â†’ convert categorical (Gender/Geography) to numeric\n",
    "* `StandardScaler` â†’ feature normalization (mean 0, std 1)\n",
    "* `Sequential` + `Dense` â†’ build ANN\n",
    "* `confusion_matrix`, `accuracy_score` â†’ evaluate model\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# 2. Load dataset\n",
    "df = pd.read_csv(\"Churn_Modelling.csv\")\n",
    "df.head()\n",
    "```\n",
    "\n",
    "Shows first 5 rows.\n",
    "\n",
    "Columns = `CustomerId`, `Surname`, `CreditScore`, `Geography`, `Gender`, `Age`, `Tenure`, `Balance`, `NumOfProducts`, `HasCrCard`, `IsActiveMember`, `EstimatedSalary`, `Exited`.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# 3. Feature and target separation\n",
    "X = df.iloc[:, 3:13].values     # features (10)\n",
    "y = df.iloc[:, 13].values       # target (Exited)\n",
    "```\n",
    "\n",
    "Skip first 3 columns (IDs not useful).\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# 4. Encode categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "le = LabelEncoder()\n",
    "X[:, 2] = le.fit_transform(X[:, 2])  # Gender: Male/Female -> 1/0\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [('encoder', OneHotEncoder(), [1])], remainder='passthrough'\n",
    ")\n",
    "X = np.array(ct.fit_transform(X))\n",
    "```\n",
    "\n",
    "**Geography** encoded as one-hot columns (France, Spain, Germany).\n",
    "`remainder='passthrough'` keeps other features unchanged.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# 5. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0\n",
    ")\n",
    "```\n",
    "\n",
    "80 % training / 20 % testing.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# 6. Feature scaling\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test  = sc.transform(X_test)\n",
    "```\n",
    "\n",
    "Needed because ANN uses gradient descent sensitive to scale.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# 7. Build the ANN\n",
    "model = Sequential()\n",
    "\n",
    "# Input + first hidden layer\n",
    "model.add(Dense(units=6, activation='relu', input_dim=12))\n",
    "# Second hidden layer\n",
    "model.add(Dense(units=6, activation='relu'))\n",
    "# Output layer\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "\n",
    "| Layer    | Units | Activation | Purpose                 |\n",
    "| -------- | ----- | ---------- | ----------------------- |\n",
    "| Hidden 1 | 6     | ReLU       | learn features          |\n",
    "| Hidden 2 | 6     | ReLU       | deeper representation   |\n",
    "| Output   | 1     | Sigmoid    | outputs probability 0â€“1 |\n",
    "\n",
    "`input_dim = 12` â†’ after one-hot encoding we have 12 features.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# 8. Compile model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "* **optimizer** = Adam â†’ variant of SGD that adapts learning rate\n",
    "* **loss** = Binary Cross-Entropy â†’ for 0/1 classification\n",
    "* **metrics** = accuracy displayed during training\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# 9. Train the model\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=50, verbose=1)\n",
    "```\n",
    "\n",
    "* **epochs** = passes through data (increase for better accuracy)\n",
    "* **batch_size = 32** â†’ gradient updates after 32 samples\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# 10. Predict on test data\n",
    "y_pred = (model.predict(X_test) > 0.5)\n",
    "```\n",
    "\n",
    "Outputs probabilities â†’ converted to Boolean predictions.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# 11. Evaluate\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Accuracy:\", acc)\n",
    "```\n",
    "\n",
    "Example Output:\n",
    "\n",
    "```\n",
    "[[1521   74]\n",
    " [ 199  206]]\n",
    "Accuracy = 0.8635\n",
    "```\n",
    "\n",
    "âœ… â‰ˆ 86 % accuracy\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ 4ï¸âƒ£ Key Concepts Inside the Code\n",
    "\n",
    "| Concept                  | Explanation                                      |\n",
    "| ------------------------ | ------------------------------------------------ |\n",
    "| **Dense Layer**          | fully connected layer                            |\n",
    "| **Activation Function**  | introduces non-linearity                         |\n",
    "| **ReLU**                 | f(x)=max(0,x) prevents vanishing gradients       |\n",
    "| **Sigmoid**              | maps output to [0,1] for probability             |\n",
    "| **Binary Cross-Entropy** | measures prediction error for binary outcomes    |\n",
    "| **Adam Optimizer**       | combines momentum + RMSProp for fast convergence |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§® 5ï¸âƒ£ Expected Results\n",
    "\n",
    "| Metric            | Typical Value              |\n",
    "| ----------------- | -------------------------- |\n",
    "| Training Accuracy | 0.85 â€“ 0.90                |\n",
    "| Test Accuracy     | 0.83 â€“ 0.87                |\n",
    "| Confusion Matrix  | ~[[1500, 100], [220, 180]] |\n",
    "\n",
    "âœ… Model predicts customer churn with ~85 % accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  6ï¸âƒ£ Possible Exam / Viva Questions & Answers\n",
    "\n",
    "| Question                             | Short Answer                                                                |\n",
    "| ------------------------------------ | --------------------------------------------------------------------------- |\n",
    "| What is an ANN?                      | Model inspired by human brain; uses interconnected neurons.                 |\n",
    "| Why normalize data?                  | To ensure faster, stable convergence of gradient descent.                   |\n",
    "| What activation did you use and why? | ReLU â†’ efficient, avoids vanishing gradients; Sigmoid â†’ probability output. |\n",
    "| Why binary_crossentropy?             | Suitable for binary classification (0/1 output).                            |\n",
    "| What does epoch mean?                | One complete pass over the training dataset.                                |\n",
    "| What does confusion matrix show?     | TP, TN, FP, FN values of predictions.                                       |\n",
    "| What optimizer was used?             | Adam â€“ adaptive learning rate algorithm.                                    |\n",
    "| What improvement can you try?        | Add dropout layer (regularization) / more neurons / change epochs.          |\n",
    "| Why use Keras Sequential model?      | Simple linear stack of layers for ANN creation.                             |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© 7ï¸âƒ£ Modifications Examiner Might Ask + How to Do Them\n",
    "\n",
    "| Task Asked                           | What to Do                           | Code Snippet                                                           |\n",
    "| ------------------------------------ | ------------------------------------ | ---------------------------------------------------------------------- |\n",
    "| **Add one more hidden layer**        | Insert another Dense layer           | `model.add(Dense(6, activation='relu'))`                               |\n",
    "| **Change activation**                | Use tanh instead of relu             | `activation='tanh'`                                                    |\n",
    "| **Use dropout to avoid overfitting** | Import & add dropout layer           | `from tensorflow.keras.layers import Dropout; model.add(Dropout(0.2))` |\n",
    "| **Change optimizer**                 | Try SGD                              | `optimizer='sgd'` in compile                                           |\n",
    "| **Show model summary**               | To view structure                    | `model.summary()`                                                      |\n",
    "| **Plot accuracy graph**              | Use matplotlib                       | `hist=model.fit(...); plt.plot(hist.history['accuracy']);`             |\n",
    "| **Show ROC curve**                   | Use sklearn.metrics.roc_curve        | Example: `fpr,tpr,_=roc_curve(y_test,y_prob)`                          |\n",
    "| **Predict for single input**         | Use `model.predict([scaled_sample])` |                                                                        |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ˆ 8ï¸âƒ£ Evaluation Metrics and Interpretation\n",
    "\n",
    "| Metric              | Meaning                    |\n",
    "| ------------------- | -------------------------- |\n",
    "| **Accuracy â‰ˆ 0.86** | Model correct â‰ˆ 86 % times |\n",
    "| **Precision High**  | Few false positives        |\n",
    "| **Recall Moderate** | Misses some churners       |\n",
    "| **F1 â‰ˆ 0.80â€“0.83**  | Balanced performance       |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§¾ 9ï¸âƒ£ Conclusion\n",
    "\n",
    "> In this assignment, we preprocessed the bank-customer dataset, normalized the data, built an **Artificial Neural Network (ANN)** using **Keras + TensorFlow**, and evaluated it with accuracy and confusion matrix.\n",
    "> The model achieved around 85â€“88 % accuracy, successfully predicting customer churn.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—’ï¸ ğŸ”Ÿ One-Page Exam Revision Sheet\n",
    "\n",
    "**Keywords:** ANN â€“ Neuron â€“ Weights â€“ Bias â€“ Activation â€“ ReLU â€“ Sigmoid â€“ Backpropagation â€“ Normalization â€“ Binary Cross Entropy â€“ Adam Optimizer â€“ Confusion Matrix\n",
    "\n",
    "**Formulas:**\n",
    "\n",
    "* Normalization â†’ (X'=(Xâˆ’X_{\\min})/(X_{\\max}âˆ’X_{\\min}))\n",
    "* Accuracy = (TP+TN)/(TP+FP+TN+FN)\n",
    "* Precision = TP/(TP+FP)\n",
    "* Recall = TP/(TP+FN)\n",
    "\n",
    "**Remember:**\n",
    "\n",
    "* Normalize before training\n",
    "* ReLU for hidden, Sigmoid for output\n",
    "* Use 2â€“3 hidden layers, 6â€“10 neurons each\n",
    "* epochs â‰ˆ 50â€“100, batch size = 32\n",
    "* Random Seed = 0 for reproducibility\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to make a **visual diagram of the ANN architecture** (input â†’ hidden â†’ output layers with neuron counts and activations)? Itâ€™s often shown in practical viva printouts for extra marks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0f5690",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
